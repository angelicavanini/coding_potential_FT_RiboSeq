

import os
import subprocess
import pysam
from Bio import SeqIO
import glob

# Paths to files and tools
transcripts_fasta = "/zfs/jacobs/AngelicaVanini/posctrl_db3/717_merged_sequences.fasta"
ribo_seq_fastq = "/zfs/jacobs/AngelicaVanini/posctrl_db3/SRR5047771_1_trimmed.fq"
bowtie2_index_base = "717_merged_sequences_index"
aligned_sam = "aligned_reads_bowtie2.sam"
aligned_bam = "aligned_reads_bowtie2.bam"
sorted_bam = "aligned_reads_bowtie2.sorted.bam"
changed_headers_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/changed_headers"
filtered_reads_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/changed_headers_only_5bp"
final_filtered_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/noFT_justRS_5bp"
blastn_results_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/blastn_results"
after_blastn_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/after_blastN"
blastn_db_path = "/zfs/jacobs/genomes/human/Genecode_v44/gencode_v44_transcripts_filtered"

# Step 1: Index the transcripts using Bowtie2
print("Indexing transcripts...")
subprocess.run(["bowtie2-build", transcripts_fasta, bowtie2_index_base], check=True)

# Step 2: Align the ribo-seq reads to the indexed transcripts
print("Aligning reads...")
subprocess.run([
    "bowtie2", 
    "-x", bowtie2_index_base, 
    "-U", ribo_seq_fastq, 
    "-S", aligned_sam, 
    "-L", "16",  # Set seed length to 16
    "-N", "0",  # Allow no mismatches in the seed
    "-a"    #all hits
], check=True)

# Step 3: Convert SAM to BAM, sort and index BAM
print("Processing SAM/BAM files...")
subprocess.run(["samtools", "view", "-bS", aligned_sam, "-o", aligned_bam], check=True)
subprocess.run(["samtools", "sort", aligned_bam, "-o", sorted_bam], check=True)
subprocess.run(["samtools", "index", sorted_bam], check=True)

# Load the transcripts
print("Loading transcripts...")
transcripts = {record.id: str(record.seq) for record in SeqIO.parse(transcripts_fasta, "fasta")}

# Open the sorted BAM file
bamfile = pysam.AlignmentFile(sorted_bam, "rb")

# Dictionary to store reads for each transcript
transcript_reads = {transcript: [] for transcript in transcripts}

# Step 4: Extract reads for each transcript
print("Extracting reads for each transcript...")
for read in bamfile.fetch():
    if not read.is_unmapped:
        transcript = bamfile.get_reference_name(read.reference_id)
        transcript_reads[transcript].append(read)

# Step 5: Write each transcript and its reads to a separate FASTA file
print("Writing FASTA files...")
for transcript, reads in transcript_reads.items():
    with open(f"{transcript}_reads.fasta", "w") as f:
        # Write the transcript
        f.write(f">{transcript}\n{transcripts[transcript]}\n")
        
        # Write the reads
        for read in reads:
            read_seq = read.query_sequence
            read_id = read.query_name
            f.write(f">{read_id}\n{read_seq}\n")

bamfile.close()
print("Process completed successfully.")

# Step 6: Change headers of the FASTA files
print("Changing headers...")
os.makedirs(changed_headers_dir, exist_ok=True)

def change_headers(input_file, output_file):
    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
        first_line = f_in.readline().strip()
        if first_line.startswith(">"):
            new_first_header = ">ft_" + first_line[1:]  # Add "ft_" after ">"
            f_out.write(new_first_header + "\n")

            # Extract the base name from the first header
            base_name = new_first_header[4:]  # Remove ">ft_" to get the base name
            count = 1
            
            for line in f_in:
                if line.startswith(">"):
                    new_header = f">rs_{base_name}_{count}"
                    f_out.write(new_header + "\n")
                    count += 1
                else:
                    f_out.write(line)

input_files = glob.glob("*_reads.fasta")

for input_file in input_files:
    output_file = os.path.join(changed_headers_dir, os.path.basename(input_file))
    change_headers(input_file, output_file)
    print(f"Processed {input_file} and saved to {output_file}")

# Step 7: Filter reads to keep only those crossing 5bp upstream and downstream of the midpoint
print("Filtering reads...")
os.makedirs(filtered_reads_dir, exist_ok=True)

def filter_reads(input_file, output_file):
    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
        lines = f_in.readlines()
        ref_id = lines[0].strip()
        ref_seq = lines[1].strip()
        
        midpoint = len(ref_seq) // 2
        upstream = midpoint - 5
        downstream = midpoint + 5

        f_out.write(f"{ref_id}\n{ref_seq}\n")
        
        for i in range(2, len(lines), 2):
            read_id = lines[i].strip()
            read_seq = lines[i+1].strip()
            
            read_start = lines[1].find(read_seq)
            read_end = read_start + len(read_seq)

            if read_start <= upstream and read_end >= downstream:
                f_out.write(f"{read_id}\n{read_seq}\n")

input_files = glob.glob(os.path.join(changed_headers_dir, "*_reads.fasta"))

for input_file in input_files:
    output_file = os.path.join(filtered_reads_dir, os.path.basename(input_file))
    filter_reads(input_file, output_file)
    print(f"Processed {input_file} and saved to {output_file}")

# Step 8: Remove FT sequences from the FASTA files
print("Removing FT sequences...")
os.makedirs(final_filtered_dir, exist_ok=True)

def filter_fasta(input_file, output_file):
    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
        skip_next = False
        for line in f_in:
            if skip_next:
                skip_next = False
                continue
            if line.startswith(">ft_"):
                skip_next = True
                continue
            f_out.write(line)

input_files = glob.glob(os.path.join(filtered_reads_dir, "*_reads.fasta"))

for input_file in input_files:
    output_file = os.path.join(final_filtered_dir, os.path.basename(input_file))
    filter_fasta(input_file, output_file)
    print(f"Processed {input_file} and saved to {output_file}")

# Step 9: Run BlastN on the filtered files
print("Running BlastN...")
os.makedirs(blastn_results_dir, exist_ok=True)

def run_blastn(input_file, output_file, db_path):
    blastn_command = [
        "blastn",
        "-query", input_file,
        "-db", db_path,
        "-out", output_file,
        "-outfmt", "6 qseqid sseqid sstart send qcovs pident stitle"
    ]
    subprocess.run(blastn_command)

input_files = glob.glob(os.path.join(final_filtered_dir, "*_reads.fasta"))

for input_file in input_files:
    base_name = os.path.basename(input_file)
    output_file = os.path.join(blastn_results_dir, f"{os.path.splitext(base_name)[0]}_blastN.txt")
    run_blastn(input_file, output_file, blastn_db_path)
    print(f"Processed {input_file} and saved to {output_file}")

import os  # Import the os module to handle file paths and directories

# Step 10: Process BlastN results and filter out reads mapping to multiple genes
print("Processing BlastN results...")

def parse_blastn_file(blastn_file):
    """Parse the BlastN results file to identify reads that map to multiple genes with â‰¥100% coverage."""
    reads_dict = {}

    with open(blastn_file, 'r') as f:
        for line in f:
            columns = line.strip().split('\t')
            if len(columns) < 7:
                continue
            read_name = columns[0]
            second_col = columns[1]
            coverage = float(columns[4])  # Coverage percentage
            ensg_id = second_col.split('|')[1][4:16]  # Extract ENSG ID

            if coverage >= 100:
                if read_name not in reads_dict:
                    reads_dict[read_name] = set()
                reads_dict[read_name].add(ensg_id)

    return reads_dict

def identify_reads_to_discard(reads_dict):
    """Identify reads that map to multiple genes."""
    reads_to_discard = {read_name for read_name, ensg_ids in reads_dict.items() if len(ensg_ids) > 1}
    return reads_to_discard

def filter_fasta_file(fasta_file, reads_to_discard, output_file):
    """Filter out reads that map to multiple genes from the FASTA file."""
    with open(fasta_file, 'r') as original_file, open(output_file, 'w') as filtered_file:
        first_line = original_file.readline()
        filtered_file.write(first_line)  # Write the >ft header
        
        write_sequence = True
        for line in original_file:
            if line.startswith(">"):
                read_name = line[1:].strip()  # Remove the '>'
                write_sequence = read_name not in reads_to_discard
                if write_sequence:
                    filtered_file.write(line)
            else:
                if write_sequence:
                    filtered_file.write(line)

def process_files(fasta_dir, blastn_results_dir, output_dir):
    """Process each file in the FASTA directory with corresponding BlastN results."""
    os.makedirs(output_dir, exist_ok=True)

    for filename in os.listdir(fasta_dir):
        if filename.endswith("_reads.fasta"):
            base_filename = os.path.splitext(filename)[0]
            blastn_filename = f"{base_filename}_blastN.txt"
            fasta_file_path = os.path.join(fasta_dir, filename)
            blastn_file_path = os.path.join(blastn_results_dir, blastn_filename)
            output_file_path = os.path.join(output_dir, filename)

            if os.path.exists(blastn_file_path):
                reads_dict = parse_blastn_file(blastn_file_path)
                reads_to_discard = identify_reads_to_discard(reads_dict)
                filter_fasta_file(fasta_file_path, reads_to_discard, output_file_path)
                print(f"Processed {filename}, saved to {output_file_path}")
            else:
                print(f"BlastN results not found for {filename}")

# Define the directories
fasta_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/set1/changed_headers_only_5bp"
blastn_results_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/set1/blastn_results"
output_dir = "/zfs/jacobs/AngelicaVanini/posctrl_db3/set1/after_blastN_100"

# Process the files
process_files(fasta_dir, blastn_results_dir, output_dir)
print("All steps completed successfully.")
